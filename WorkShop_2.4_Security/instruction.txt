Instruction for Workshop 2.4 Kubernetes Security:
Note: This instruction will start lab for kubernetes's cluster for real workshop:
============================================================================================================================================================
Part 1: Network Policy
============================================================================================================================================================
1. Create Application Set by command:
	kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/management-ui-set.yml
	kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/backend-set.yml
	kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/frontend-set.yml
	kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/client-set.yml

2. Check Namespace / Service / Pods 
	kubectl get namespace
	kubectl get all -n=management-ui 
	kubectl get all -n=stars
	kubectl get all -n=client

3. Test open browser for check application:
	http://<Public IP>:32500

4. Apply network policy for denied any connection to namespace: "stars" and "client"
	kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/policy-deny-client.yml
	kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/policy-deny-stars.yml

5. Test open browser again:
	http://<Public IP>:32500

6. Apply network policy for allow all pods from namespace "management-ui" (label: role=management-ui) access to any pods in namespace "stars"
	kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/policy-allow-ui.yml

7. Apply network policy for allow all pods from namespace "management-ui" (label: role=management-ui) access to any pods in namespace "client"
	kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/policy-allow-ui-client.yml

8. Test open browser again:
	http://<Public IP>:32500

9. Apply network policy for allow pods frontend (label: role=frontend) to pods backend (label: role=backend) in same namespace
	kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/policy-allow-backend.yml

10. Apply network policy for allow all pods from namespace "client" (label: role=client) access to pods frontend (label: role=frontend)
	kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/policy-allow-frontend.yml

11. Test open browser again:
	http://<Publi IP>:32500

12. CleanUp Lab by command:
	kubectl delete -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/management-ui-set.yml
	kubectl delete -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/backend-set.yml
	kubectl delete -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/frontend-set.yml
	kubectl delete -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/client-set.yml

============================================================================================================================================================
Part 2: Access Control Policy (User Account)
============================================================================================================================================================

1. Create namespace "security" and deploy application via command:
	kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/security-namespace.yml
	kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/security-pod.yml
	kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/security-svc.yml
	kubectl get pod,svc -n=security
	curl http://<Private IP>:32500

2. Configure Openssl and Create private key for new user account
cd ~
sudo sed -i '/RANDFILE/d' /etc/ssl/openssl.cnf
openssl genrsa -out labreadonly.key 2048

3. Create CSR from private key (User: labreadonly, OU: labdockerthailand)
openssl req -new -key labreadonly.key -out labreadonly.csr -subj "/CN=labreadonly/O=labdockerthailand"

4. Generate Certificate base on CSR with aging 365 days
sudo openssl x509 -req -in labreadonly.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out labreadonly.crt -days 365

5. Private key and Public key will place on /home/ubuntu (~)
ls ~/labreadonly*

6. Setup credential context via kubectl by command:
cd ~
cp ~/.kube/config ~/.kube/config_backup
kubectl config set-credentials labreadonly --client-certificate=/home/ubuntu/labreadonly.crt  --client-key=/home/ubuntu/labreadonly.key
kubectl config set-context labreadonly-context --cluster=kuberneteslab --namespace=security --user=labreadonly

7. Test use credential for operate by command: (Expect: Forbidden)
kubectl --context=labreadonly-context get pods

8. Create Role "ReadOnly" by command:
kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/security-role-readonly.yml
kubectl get roles -n=security

9. Create RoleBinding for user "labreadonly" with role "rolereadonly"
kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/security-rolebinding-labsecurity.yml
kubectl get rolebinding -n=security

10. Try to get pods again.
kubectl --context=labreadonly-context get pods

11. Try to get svc,delete pods/create new pods: (Expect: Forbidden)
kubectl --context=labreadonly-context get svc
kubectl --context=labreadonly-context delete pods/webtest
kubectl --context=labreadonly-context run webtest --image=labdocker/nginx:http2 --port=443 -n=security

12. CleanUp Lab:
kubectl delete -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/security-rolebinding-labsecurity.yml
kubectl config delete-context labreadonly-context

============================================================================================================================================================
Part 3: Access Control Policy (System Account)
============================================================================================================================================================
1. Create Service Account by command:
kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/security-serviceaccount-readonly.yml
kubectl get sa -n=security

2. Create RoleBinding by command:
kubectl create rolebinding rolebindingserviceaccountreadonly \
  --role=rolereadonly \
  --serviceaccount=security:serviceaccount-readonly \
  --namespace=security

3. Test Service Account by command:
kubectl get pods -n security --as system:serviceaccount:security:serviceaccount-readonly
kubectl get svc -n security --as system:serviceaccount:security:serviceaccount-readonly
kubectl run webtest --image=labdocker/nginx:http2 --port=443 -n security --as system:serviceaccount:security:serviceaccount-readonly

4. Clearup Lab by command:
kubectl delete rolebinding/rolebindingserviceaccountreadonly -n=security
kubectl delete -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/security-serviceaccount-readonly.yml
kubectl delete -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/security-role-readonly.yml
kubectl delete -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/security-pod.yml
kubectl delete -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/security-svc.yml
kubectl delete -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/security-namespace.yml

============================================================================================================================================================
Part 4: Pods Security Association (PSA)
============================================================================================================================================================
1. Create Namespace with enforce PSA by command:
kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/psa-namespace.yml

2. Check detail of namespace by command:
kubectl describe namespace/namespace-psa

3. Create deployment pods with hostpath (Expect: Failure by enforce/baseline) by command:
kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/psa-hostpath-pods.yml

4. Create deployment with standard pods by command (Expect: Get warning message by warning/restrict):
kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/psa-normal-pods.yml

5. Access to pods and check user id and file permission by command:
kubectl get pods -n=namespace-psa
kubectl exec -it alpineweb sh -n=namespace-psa
ps 											==> record USER
mkdir /data && touch /data/testfile			
ls /data/ 									
ping 1.1.1.1
apk update && apk add curl
curl https://www.google.com
id 											==> record ID
exit

6. CleanUp Lab by command:
kubectl delete -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/psa-normal-pods.yml
kubectl delete -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/psa-namespace.yml

============================================================================================================================================================
Part 5: Encryption Provider
============================================================================================================================================================
1. Verify Encryption by command:
more ~/kubernetes_202312/WorkShop_2.4_Security/enc.yml
----------------------------------------------------
Example output:
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
      - secrets
    providers:
      - secretbox:
          keys:
            - name: key1
              secret: XXXXXXXXXXXXX
      - identity: {} # default encryption provider to allow reading unencrypted secrets
----------------------------------------------------

2. Install etcd client by command:
sudo apt-get update && sudo apt-get install etcd-client

3. Deploy application stack use default secret/configmap as command below:
  3.1. Create Namespace by command:
	kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_1.8_ConfigMap_Secret/webmicroservice_namespace.yml

  3.2. Create ConfigMap by command:
	kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_1.8_ConfigMap_Secret/webmodule_configmap.yml
	kubectl get configmap -n webmicroservice
	kubectl describe configmap/webmodule-configmap -n webmicroservice

  3.3. Create Secret by command:
	kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_1.8_ConfigMap_Secret/databasemodule_secret.yml
	kubectl get secret -n webmicroservice
	kubectl describe secret/databasemodule-secret -n webmicroservice

  3.4 Create Database Deployment by command:
	kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_1.8_ConfigMap_Secret/databasemodule_deploy_config.yml
	kubectl get deployment -n webmicroservice
	kubectl get rs -n webmicroservice
	kubectl get pods -n webmicroservice 

4. Check location of etcd datastore for Kubernetes cluster by command:
sudo cat /etc/kubernetes/manifests/etcd.yaml
----------------------------------------------------
Record Path as below
	- cert-file = <cert path> (Default path: /etc/kubernetes/pki/etcd/server.crt)
	- key-file = <key path> (Default path: /etc/kubernetes/pki/etcd/server.key)
	- trust-ca-file = <trust-ca path> (Default path: /etc/kubernetes/pki/etcd/ca.crt)
----------------------------------------------------

5. Test get value of secret and configmap on etcd datastore by command:
sudo ETCDCTL_API=3 etcdctl \
   --cert=/etc/kubernetes/pki/etcd/server.crt \
   --key=/etc/kubernetes/pki/etcd/server.key \
   --cacert=/etc/kubernetes/pki/etcd/ca.crt \
   get /registry/secrets/webmicroservice/databasemodule-secret

6. Hack to see etcd database file directly by command:
	sudo cat /var/lib/etcd/member/snap/db | more

7. Download file enc.yml and place on "/etc/kubernetes/enc/enc.yml" by command:
  sudo su -
  mkdir /etc/kubernetes/enc/
	curl https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_2.4_Security/enc.yml > /etc/kubernetes/enc/enc.yml
  more /etc/kubernetes/enc/enc.yml
  exit

8. Enhance encryption provider on path "/etc/kubernetes/manifests/kube-apiserver.yaml" and edit file as detail below
  sudo su -
  vi /etc/kubernetes/manifests/kube-apiserver.yaml
---
#
# This is a fragment of a manifest for a static Pod.
# Check whether this is correct for your cluster and for your API server.
#
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: X.X.X.X:443
  creationTimestamp: null
  labels:
    app.kubernetes.io/component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    ...
    - --encryption-provider-config=/etc/kubernetes/enc/enc.yml  # add this line
    volumeMounts:
    ...
    - name: enc                           # add this line
      mountPath: /etc/kubernetes/enc      # add this line
      readOnly: true                      # add this line
    ...
  volumes:
  ...
  - name: enc                             # add this line
    hostPath:                             # add this line
      path: /etc/kubernetes/enc           # add this line
      type: DirectoryOrCreate             # add this line
  ...
---

9. Reboot server for apply new configuration by command: sudo shutdown -r now

10. Relogin server and check health of all kube-system pods by command: kubectl get pods -n=kube-system

11. Verify the change of encryption-provider-config by command: sudo ps -ef | grep kube-apiserver | grep "encryption-provider-config"

12. Update all secret with default config with the new by command: kubectl get secrets -n=webmicroservice -o json | kubectl replace -f -

13. Test get value of secret and configmap on etcd datastore by command:
sudo ETCDCTL_API=3 etcdctl \
   --cert=/etc/kubernetes/pki/etcd/server.crt \
   --key=/etc/kubernetes/pki/etcd/server.key \
   --cacert=/etc/kubernetes/pki/etcd/ca.crt \
   get /registry/secrets/webmicroservice/databasemodule-secret

15. Hack to see etcd database file directly by command:
	sudo cat /var/lib/etcd/member/snap/db | more

16. Cleanup lab by command:
	kubectl delete -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_1.8_ConfigMap_Secret/databasemodule_deploy_config.yml
	kubectl delete -f https://raw.githubusercontent.com/praparn/kubernetes_202312/master/WorkShop_1.8_ConfigMap_Secret/webmicroservice_namespace.yml
